# LLM Dispatch Tool for Local AI Research

This is a wrapper for local AI tool Ollama that supports file embedding and RAG. Tool is still under development but you can run it locally with the following instructions.

Make sure that you have Ollama installed on your machine. You can find the installation instructions [here](https://ollama.com/download).

## Getting Started

To run this application:

First run the following command to install the dependencies:

```bash
npm install
```

Then run the following command to start the application:

```bash
npm run dev
```

## Building For Production

To build this application for production:

```bash
npm run build
```
